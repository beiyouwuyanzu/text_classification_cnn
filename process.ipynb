{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 简介\n",
    "## paddlepaddle论文复现\n",
    "- https://aistudio.baidu.com/aistudio/competition/detail/106\n",
    "### 复现论文： 字符级CNN文本分类\n",
    "- 论文名称： Character-level Convolutional Networks for Text Classification\n",
    "- 数据集： AG News Dbpedia Yelp Binary classification Yelp Fine-grained classification\n",
    "- 验收标准： \n",
    "\t1. 复现论文中“Lg. Conv. Th.”模型（见Table 4）（参考论文和实现链接） \n",
    "    \n",
    "   \t\t- \thttps://arxiv.org/pdf/1509.01626v3.pdf\n",
    "    \t-   https://github.com/gaussic/text-classification-cnn-rnn\n",
    "      \n",
    "\t2. Amazon Review Full测试集error rate=40.45%，Yahoo! Answers测试集error rate=28.80%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 处理输入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "体育\t马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军记者傅亚雨沈阳报道 来到沈阳，国奥队依然没有摆脱雨水的困扰。7月31日下午6点，国奥队的日常训练再度受到大雨的干扰，无奈之下队员们只慢跑了25分钟就草草收场。31日上午10点，国奥队在奥体中心外场训练的时候，天就是阴沉沉的，气象预报显示当天下午沈阳就有大雨，但幸好队伍上午的训练并没有受到任何干扰。下午6点，当球队抵达训练场时，大雨已经下了几个小时，而且丝毫没有停下来的意思。抱着试一试的态度，球队开始了当天下午的例行训练，25分钟过去了，天气没有任何转好的迹象，为了保护球员们，国奥队决定中止当天的训练，全队立即返回酒店。在雨中训练对足球队来说并不是什么稀罕事，但在奥运会即将开始之前，全队变得“娇贵”了。在沈阳最后一周的训练，国奥队首先要保证现有的球员不再出现意外的伤病情况以免影响正式比赛，因此这一阶段控制训练受伤、控制感冒等疾病的出现被队伍放在了相当重要的位置。而抵达沈阳之后，中后卫冯萧霆就一直没有训练，冯萧霆是7月27日在长春患上了感冒，因此也没有参加29日跟塞尔维亚的热身赛。队伍介绍说，冯萧霆并没有出现发烧症状，但为了安全起见，这两天还是让他静养休息，等感冒彻底好了之后再恢复训练。由于有了冯萧霆这个例子，因此国奥队对雨中训练就显得特别谨慎，主要是担心球员们受凉而引发感冒，造成非战斗减员。而女足队员马晓旭在热身赛中受伤导致无缘奥运的前科，也让在沈阳的国奥队现在格外警惕，“训练中不断嘱咐队员们要注意动作，我们可不能再出这样的事情了。”一位工作人员表示。从长春到沈阳，雨水一路伴随着国奥队，“也邪了，我们走到哪儿雨就下到哪儿，在长春几次训练都被大雨给搅和了，没想到来沈阳又碰到这种事情。”一位国奥球员也对雨水的“青睐”有些不解。\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 1 work/data/cnews.train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD>\r\n",
      "，\r\n",
      "的\r\n",
      "。\r\n",
      "一\r\n",
      "是\r\n",
      "在\r\n",
      "0\r\n",
      "有\r\n",
      "不\r\n"
     ]
    }
   ],
   "source": [
    "! head  work/data/cnews.vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999 work/data/cnews.vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l work/data/cnews.vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\r\n",
    "from collections import Counter\r\n",
    "\r\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<PAD>', 0), ('，', 1), ('的', 2), ('。', 3), ('一', 4), ('是', 5), ('在', 6), ('0', 7), ('有', 8), ('不', 9), ('了', 10), ('中', 11), ('1', 12), ('人', 13), ('大', 14), ('、', 15), ('国', 16), ('', 3903), ('2', 18), ('这', 19)]\n"
     ]
    }
   ],
   "source": [
    "# 将词转化成id\r\n",
    "def read_vocab(vocab_dir):\r\n",
    "    \"\"\"读取词汇表\"\"\"\r\n",
    "    # words = open_file(vocab_dir).read().strip().split('\\n')\r\n",
    "    with open(vocab_dir) as fp:\r\n",
    "        # 如果是py2 则每个值都转化为unicode\r\n",
    "        words = [_.strip() for _ in fp.readlines()]\r\n",
    "    word_to_id = dict(zip(words, range(len(words))))\r\n",
    "    return words, word_to_id\r\n",
    "\r\n",
    "words, word_dict = read_vocab('work/data/cnews.vocab.txt')\r\n",
    "\r\n",
    "print(list(word_dict.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 生成批次数据\r\n",
    "\r\n",
    "def batch_iter(x, y, batch_size=64):\r\n",
    "    \"\"\"生成批次数据\"\"\"\r\n",
    "    data_len = len(x)\r\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\r\n",
    "\r\n",
    "    indices = np.random.permutation(np.arange(data_len))\r\n",
    "    x_shuffle = x[indices]\r\n",
    "    y_shuffle = y[indices]\r\n",
    "\r\n",
    "    for i in range(num_batch):\r\n",
    "        start_id = i * batch_size\r\n",
    "        end_id = min((i + 1) * batch_size, data_len)\r\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('体育', 0), ('财经', 1), ('房产', 2), ('家居', 3), ('教育', 4), ('科技', 5), ('时尚', 6), ('时政', 7), ('游戏', 8), ('娱乐', 9)]\n"
     ]
    }
   ],
   "source": [
    "# 编码label\r\n",
    "\r\n",
    "def read_category():\r\n",
    "    \"\"\"读取分类目录，固定\"\"\"\r\n",
    "    categories = ['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\r\n",
    "\r\n",
    "    categories = [x for x in categories]\r\n",
    "\r\n",
    "    cat_to_id = dict(zip(categories, range(len(categories))))\r\n",
    "\r\n",
    "    return categories, cat_to_id\r\n",
    "\r\n",
    "categories, cat_dict = read_category()\r\n",
    "print(list(cat_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 编码输入输出文件\r\n",
    "\r\n",
    "# 朝前面补充0\r\n",
    "def pad(arr, l):\r\n",
    "    if len(arr) > l:\r\n",
    "        return arr[:l]\r\n",
    "    else:\r\n",
    "        return [0 for _ in range(l - len(arr))] + arr\r\n",
    "\r\n",
    "# one hot 编码\r\n",
    "def one_hot(pos, l):\r\n",
    "    res = [0 for _ in range (l)]\r\n",
    "    res[pos] = 1\r\n",
    "    return res\r\n",
    "\r\n",
    "def read_file(filename):\r\n",
    "    \"\"\"读取文件数据\"\"\"\r\n",
    "    contents, labels = [], []\r\n",
    "    with open(filename) as f:\r\n",
    "        for line in f:\r\n",
    "            try:\r\n",
    "                label, content = line.strip().split('\\t')\r\n",
    "                if content:\r\n",
    "                    contents.append(list(content))\r\n",
    "                    labels.append(label)\r\n",
    "            except:\r\n",
    "                pass\r\n",
    "    return contents, labels\r\n",
    "\r\n",
    "\r\n",
    "def process_file(filename, word_to_id, cat_to_id, max_length=600):\r\n",
    "    \"\"\"将文件转换为id表示\"\"\"\r\n",
    "    contents, labels = read_file(filename)\r\n",
    "\r\n",
    "    data_id, label_id = [], []\r\n",
    "    for i in range(len(contents)):\r\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\r\n",
    "        label_id.append(cat_to_id[labels[i]])\r\n",
    "        # if i == 1000:\r\n",
    "        #     break\r\n",
    "\r\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\r\n",
    "    # print(data_id[:3])\r\n",
    "    # print(label_id[:3])\r\n",
    "    # x_pad = pad(data_id, max_length)\r\n",
    "    x_pad = [pad(x, max_length) for x in data_id]\r\n",
    "    # x_pad = map(data_id, pad(l = max_length))\r\n",
    "    y_pad = [one_hot(x, len(cat_to_id)) for x in label_id]\r\n",
    "    # y_pad = map(label_id, one_hot(l = len(cat_to_id)))\r\n",
    "    # y_pad = one_hot(label_id, len(cat_to_id))  # 将标签转换为one-hot表示\r\n",
    "    return x_pad, y_pad\r\n",
    "\r\n",
    "\r\n",
    "train_x, train_y = process_file('work/data/cnews.train.txt', word_dict, cat_dict )\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_y[9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型构造\n",
    "### 原始模型代码\n",
    "```python\n",
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "\n",
    "class TCNNConfig(object):\n",
    "    \"\"\"CNN配置参数\"\"\"\n",
    "\n",
    "    embedding_dim = 64  # 词向量维度\n",
    "    seq_length = 600  # 序列长度\n",
    "    num_classes = 10  # 类别数\n",
    "    num_filters = 256  # 卷积核数目\n",
    "    kernel_size = 5  # 卷积核尺寸\n",
    "    vocab_size = 5000  # 词汇表达小\n",
    "\n",
    "    hidden_dim = 128  # 全连接层神经元\n",
    "\n",
    "    dropout_keep_prob = 0.5  # dropout保留比例\n",
    "    learning_rate = 1e-3  # 学习率\n",
    "\n",
    "    batch_size = 64  # 每批训练大小\n",
    "    num_epochs = 10  # 总迭代轮次\n",
    "\n",
    "    print_per_batch = 100  # 每多少轮输出一次结果\n",
    "    save_per_batch = 10  # 每多少轮存入tensorboard\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"文本分类，CNN模型\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        # 三个待输入的数据\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        self.cnn()\n",
    "\n",
    "    def cnn(self):\n",
    "        \"\"\"CNN模型\"\"\"\n",
    "        # 词向量映射\n",
    "        with tf.device('/cpu:0'):\n",
    "            embedding = tf.get_variable('embedding', [self.config.vocab_size, self.config.embedding_dim])\n",
    "            embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n",
    "\n",
    "        with tf.name_scope(\"cnn\"):\n",
    "            # CNN layer\n",
    "            conv = tf.layers.conv1d(embedding_inputs, self.config.num_filters, self.config.kernel_size, name='conv')\n",
    "            # global max pooling layer\n",
    "            gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')\n",
    "\n",
    "        with tf.name_scope(\"score\"):\n",
    "            # 全连接层，后面接dropout以及relu激活\n",
    "            fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1')\n",
    "            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n",
    "            fc = tf.nn.relu(fc)\n",
    "\n",
    "            # 分类器\n",
    "            self.logits = tf.layers.dense(fc, self.config.num_classes, name='fc2')\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)  # 预测类别\n",
    "\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            # 损失函数，交叉熵\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            # 优化器\n",
    "            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # 准确率\n",
    "            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "```\n",
    "\n",
    "### 原始模型结构\n",
    "\n",
    "---------\n",
    "|Variables: name (type shape) |[size]|\n",
    "|-|-|\n",
    "|embedding:0| (float32_ref 5000x64) [320000, bytes: 1280000]\n",
    "|conv/kernel:0| (float32_ref 5x64x256) [81920, bytes: 327680]\n",
    "|conv/bias:0| (float32_ref 256) [256, bytes: 1024]\n",
    "|fc1/kernel:0| (float32_ref 256x128) [32768, bytes: 131072]\n",
    "|fc1/bias:0| (float32_ref 128) [128, bytes: 512]\n",
    "|fc2/kernel:0| (float32_ref 128x10) [1280, bytes: 5120]\n",
    "|fc2/bias:0| (float32_ref 10) [10, bytes: 40]\n",
    "|Total size of variables: |436362\n",
    "|Total bytes of variables: |1745448\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 新建模型\r\n",
    "# 定义 SimpleNet 网络结构\r\n",
    "import paddle\r\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear,Conv1D,max_pool1d\r\n",
    "import paddle.nn.functional as F\r\n",
    "import paddle.fluid as fluid\r\n",
    "\r\n",
    "\r\n",
    "#config\r\n",
    "class TCNNConfig(object):\r\n",
    "    \"\"\"CNN配置参数\"\"\"\r\n",
    "\r\n",
    "    embedding_dim = 64  # 词向量维度\r\n",
    "    seq_length = 600  # 序列长度\r\n",
    "    num_classes = 10  # 类别数\r\n",
    "    num_filters = 256  # 卷积核数目\r\n",
    "    kernel_size = 5  # 卷积核尺寸\r\n",
    "    vocab_size = 5000  # 词汇表达小\r\n",
    "\r\n",
    "    hidden_dim = 128  # 全连接层神经元\r\n",
    "\r\n",
    "    dropout_keep_prob = 0.5  # dropout保留比例\r\n",
    "    learning_rate = 1e-3  # 学习率\r\n",
    "\r\n",
    "    batch_size = 64  # 每批训练大小\r\n",
    "    num_epochs = 10  # 总迭代轮次\r\n",
    "\r\n",
    "    print_per_batch = 100  # 每多少轮输出一次结果\r\n",
    "    save_per_batch = 10  # 每多少轮存入tensorboard\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# 多层卷积神经网络实现\r\n",
    "class CNN(paddle.nn.Layer):\r\n",
    "     def __init__(self, config):\r\n",
    "         super(CNN, self).__init__()\r\n",
    "         self.config = config\r\n",
    "\r\n",
    "         #embedding\r\n",
    "         self.embedding = fluid.layers.embedding(input = inputs, size = [self.config.vocab_size, self.config.embedding_dim])\r\n",
    "         # 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2\r\n",
    "         self.conv1 = Conv1D(in_channels= config.embedding_dim, out_channels= config.num_filters, kernel_size= config.kernel_size)\r\n",
    "         # 定义池化层，池化核的大小kernel_size为2，池化步长为2\r\n",
    "         self.max_pool1 = max_pool1d(kernel_size=64, stride=0)\r\n",
    "         # 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2\r\n",
    "         self.conv2 = Conv2D(in_channels=20, out_channels=20, kernel_size=5, stride=1, padding=2)\r\n",
    "         # 定义池化层，池化核的大小kernel_size为2，池化步长为2\r\n",
    "         self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\r\n",
    "         # 定义一层全连接层，输出维度是1\r\n",
    "         self.fc = Linear(in_features=980, out_features=1)\r\n",
    "         \r\n",
    "    # 定义网络前向计算过程，卷积后紧接着使用池化层，最后使用全连接层计算最终输出\r\n",
    "    # 卷积层激活函数使用Relu，全连接层不使用激活函数\r\n",
    "     def forward(self, inputs):\r\n",
    "         x = self.conv1(inputs)\r\n",
    "         x = F.relu(x)\r\n",
    "         x = self.max_pool1(x)\r\n",
    "         x = self.conv2(x)\r\n",
    "         x = F.relu(x)\r\n",
    "         x = self.max_pool2(x)\r\n",
    "         x = paddle.reshape(x, [x.shape[0], -1])\r\n",
    "         x = self.fc(x)\r\n",
    "         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'text_classification_cnn'...\n",
      "remote: Enumerating objects: 6, done.\u001b[K\n",
      "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (6/6), done.\n",
      "Checking connectivity... done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/beiyouwuyanzu/text_classification_cnn.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: unable to access 'https://github.com/beiyouwuyanzu/text_classification_cnn.git/': gnutls_handshake() failed: Error in the pull function.\r\n"
     ]
    }
   ],
   "source": [
    "!cd text_classification_cnn/ && git push origin HEAD:refs/for/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.2 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
